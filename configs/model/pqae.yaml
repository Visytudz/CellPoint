_target_: cellpoint.models.PointPQAE

config:
  name: pqae
  shared:
    # ----------------------------------------------------------------
    # Shared variables for reuse across different modules
    # ----------------------------------------------------------------
    embed_dim: 384          # Main embedding dimension for the transformer blocks
    group_size: 32          # Number of points in each local patch (group)
    in_channels: 3          # Input dimension of points (X, Y, Z)
    mlp_ratio: 4.0          # Ratio for the hidden dimension in MLP blocks
    qkv_bias: True          # Whether to use bias in query, key, value projections
    proj_drop: 0.0          # Dropout rate for projection layers
    attn_drop: 0.0          # Dropout rate for attention layers
    decoder_num_heads: 6    # Number of attention heads for decoder-related parts
    drop_path_rate: 0.1

  # ----------------------------------------------------------------
  # Parameters for each specific module
  # ----------------------------------------------------------------
  view_generator:
    min_crop_rate: 0.6
    max_crop_rate: 1.0

  grouping:
    num_group: 64
    group_size: ${..shared.group_size} # Reusing shared variable

  patch_embed:
    in_channels: ${..shared.in_channels} # Reusing shared variable
    embed_dim: ${..shared.embed_dim}     # Reusing shared variable

  encoder:
    embed_dim: ${..shared.embed_dim}
    depth: 12
    num_heads: 6
    mlp_ratio: ${..shared.mlp_ratio}
    qkv_bias: ${..shared.qkv_bias}
    proj_drop: ${..shared.proj_drop}
    attn_drop: ${..shared.attn_drop}
    drop_path_rate: ${..shared.drop_path_rate}

  positional_query:
    embed_dim: ${..shared.embed_dim}
    num_heads: ${..shared.decoder_num_heads} # Using decoder-specific head count
    qkv_bias: ${..shared.qkv_bias}
    proj_drop: ${..shared.proj_drop}
    attn_drop: ${..shared.attn_drop}

  reconstruction_head:
    embed_dim: ${..shared.embed_dim}
    depth: 4
    num_heads: ${..shared.decoder_num_heads} # Using decoder-specific head count
    group_size: ${..shared.group_size}
    mlp_ratio: ${..shared.mlp_ratio}
    qkv_bias: ${..shared.qkv_bias}
    proj_drop: ${..shared.proj_drop}
    attn_drop: ${..shared.attn_drop}
    drop_path_rate: 0.0
    C_out: ${..shared.in_channels} # Output dimension should match input